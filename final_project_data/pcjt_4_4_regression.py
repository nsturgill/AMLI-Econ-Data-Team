# -*- coding: utf-8 -*-
"""Pcjt:4/4 Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhIFfpx5dtbNrDJset9Q4TqDqRIQ1gjV
"""

import pandas as pd
import numpy as np

df = pd.read_csv("Dropbox/AMLI/final_project_data/features_and_response.csv")
df = pd.DataFrame(data=df)

df

df.columns
#df.head()

df.describe()
#df.shape

#Scale SizeRank column for regression
from sklearn.preprocessing import scale
df['SizeRank'] = scale(df['SizeRank'])

#playing around to understand iloc
test_df = df.iloc[:,2:45]
test_df.head()

#Shuffle data
rand_df = df.reindex(np.random.permutation(df.index))
rand_df.head()

rand_df.describe()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error

#Test train split
x = rand_df.iloc[:, 2:45]
y = rand_df['Estimate; INCOME AND BENEFITS (IN 2017 INFLATION-ADJUSTED DOLLARS) - Total households - Median household income (dollars)']

#perform regression
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
lin_reg = LinearRegression()
lin_reg.fit(x_train, y_train)
print(lin_reg.score(x_train,y_train))

# #Logistic Regression              ???????? This is because my response varaible is continious. Needs to be categorical. Good one for icome brackets. 
# log_reg = LogisticRegression()
# log_reg.fit(x_train, y_train)
# #print(log_reg.score(x_train, y_train))

y_pred = lin_reg.predict(x_test)

#Evaluate Model: get root mean squared error between predictions and actuals

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred)

import math
math.sqrt(mean_squared_error(y_test, y_pred))

#was able to predict y (median household income) +- $13,8459.88

featurecols = x.columns
print(lin_reg.intercept_)
print(lin_reg.coef_)
list(zip(featurecols, lin_reg.coef_))

#Feature Selection with RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
rfe = RFE(lin_reg)
fit = rfe.fit(x,y)
#Number of features selected: 21 
print("Num of Features: " + str(fit.n_features_))
#Which columns were chosen by RFE 
list(zip(featurecols,fit.support_))

#Ranking of features                  ????
# Couldn't get that to work        
#fit.support_

#Feature Selection with PCA
from sklearn.decomposition import PCA
# feature extraction
pca = PCA(n_components = 3)
fit = pca.fit(x)
print("Explained Variance: " + str(fit.explained_variance_ratio_))

print(fit.components_)
pca.transform(x)
# summarize components
# print("Explained Variance: %s") % fit.explained_variance_ratio_
# print(fit.components_)

#PCA transforms the data set using the "new" features it created, then that model can be fed into the classification ML model.

# # Feature Extraction with PCA
# import numpy
# from pandas import read_csv
# from sklearn.decomposition import PCA
# # load data
# url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
# dataframe = read_csv(url, names=names)
# array = dataframe.values
# X = array[:,0:8]
# Y = array[:,8]
# # feature extraction
# pca = PCA(n_components=3)
# fit = pca.fit(X)
# # summarize components
# print("Explained Variance: %s") % fit.explained_variance_ratio_
# print(fit.components_)

x.shape

#SelectKBest(score_func = lambda x, y:  )
from sklearn.feature_selection import SelectKBest, chi2

X_new = SelectKBest(k=20).fit_transform(x, y)
X_new.shape
list(X_new)

X_new2 = SelectKBest(k=20).get_params()
X_new2

#####ANOVA RESULTS##########


from sklearn.feature_selection import f_classif

#use f test and anova to select the 21 best features 
fvalue_selector = SelectKBest(f_classif, k=22)

#createing an array with data from only the 21 features
X_kbest = fvalue_selector.fit_transform(x,y)

print(X_kbest.shape)

#what are the columns of the 21 best features 
x.columns[fvalue_selector.get_support(indices = True)]

# Create an SelectKBest object to select features with two best ANOVA F-Values
fvalue_selector = SelectKBest(f_classif, k=2)

# Apply the SelectKBest object to the features and target
X_kbest = fvalue_selector.fit_transform(x, y)
#View Results
# Show results
print('Original number of features:', x.shape[1])
print('Reduced number of features:', X_kbest.shape[1])

x.columns[fvalue_selector.get_support(indices=True)]





l_reg = Lasso()
l_reg.fit(x_train, y_train)
rsquared = round(l_reg.score(x_train,y_train),4)

rfel = RFE(l_reg)
fit = rfel.fit(x,y)
#Number of features selected: 21 
print("Num of Features: " + str(fit.n_features_))
#Which columns were chosen by RFE 
temp = pd.DataFrame(list(zip(featurecols,fit.support_)))
temp = temp[temp[1]==True]


